[eval]
data-directory = /home/esteves/dne5/github/defacto/defacto-core/models/fact/
train-directory = eval/train
test-directory = eval/test

[boa]
; how many boa patterns should be used to search: N * NUMBER_OF_BOA_PATTERNS queries and search results will be generated
NUMBER_OF_BOA_PATTERNS = 5
; only patterns which have a pattern score higher than this will be used to search for fact in text
PATTERN_SCORE_THRESHOLD = 0.5
languages = de,en,fr

[crawl]
; the bing api key get your own at: 
; BING_API_KEY = vm+K+uxrwn3oRap0Ou87KePKjGzH3gLGFM1vMw776wc=
BING_API_KEY = grgZ1JV8G6y/zbIVA9DChAuOuUIjdcsHoxhTkz2MXKQ
; number of search results return be the search engine
NUMBER_OF_SEARCH_RESULTS = 20
; thredas
NUMBER_OF_SEARCH_RESULTS_THREADS = 40
; crawl thread gets terminated after this time
WEB_SEARCH_TIMEOUT_MILLISECONDS = 5000
; One thread for every BOA pattern, usually the same as the number of BOA patterns 
NUMBER_OF_SEARCH_THREADS = 100
; the amount of time which lies between a google page rank request (in ms)
GOOGLE_WAIT_TIME = 2000
; get the page rank or not
getPageRank = false
; solr_searchresults	= http://139.18.2.164:8080/solr/factbench-sr
; solr_topicterms	    = http://139.18.2.164:8080/solr/factbench-tt
; solr_boa_fr		= http://139.18.2.164:8080/solr/boa_fr
; solr_boa_de 		= http://139.18.2.164:8080/solr/boa_de
; solr_boa_en 		= http://139.18.2.164:8080/solr/boa_en
solr_searchresults	= http://localhost:8081/solr/factbench_searchresults/
solr_topicterms	    = http://localhost:8081/solr/factbench_topicterms/
solr_boa_fr = http://localhost:8081/solr/boa_fr/
solr_boa_de = http://localhost:8081/solr/boa_de/
solr_boa_en = http://localhost:8081/solr/boa_en/





[server]
; ip = http://139.18.2.164/
; port = 1234

ip = http://localhost/
port = 8081

[extract]
; when we search for sentence which contain two labels, this is the amount of tokens between them
NUMBER_OF_TOKENS_BETWEEN_ENTITIES = 20
; the number of stanford models which get loaded on system start
NUMBER_NLP_STANFORD_MODELS = 50

[fact]
; write the training examples in this file
FACT_TRAINING_DATA_FILENAME = defacto_fact.arff
; LINEAR_REGRESSION_CLASSIFIER or MULTILAYER_PERCEPTRON or SMO
FACT_CLASSIFIER_TYPE = 66_33_proof_smo_reg_polykernel.model
ARFF_TRAINING_DATA_FILENAME = 66_33_proof_smo_reg_polykernel.arff
; do we want to write the fact confirmation weka training file
OVERWRITE_FACT_TRAINING_FILE = false


[evidence]
; LINEAR_REGRESSION_CLASSIFIER or MULTILAYER_PERCEPTRON or SMO
EVIDENCE_CLASSIFIER_TYPE = j48_ml_.model
; write the training examples in this file
EVIDENCE_TRAINING_DATA_FILENAME = defacto_evidence.arff
; active this flag to rewrite the training data
OVERWRITE_EVIDENCE_TRAINING_FILE = false
; see the paper Nakamura et. al. 2007 for details 
WEBSITE_SIMILARITY_THRESHOLD = 0.8
; onyl this much results will be returned from wikipedia topic term query: "barack obama michelle obama" returns 860 results
MAX_WIKIPEDIA_RESULTS = 10
; use only the n most frequent topic terms from wikipedia pages
TOPIC_TERMS_MAX_SIZE = 10
; one topic term needs to be in more than this pages
TOPIC_TERM_PAGE_THRESHOLD = 1
; set this flag if you want to override the current trained model
OVERWRITE_EVIDENCE_MODEL = false
; no google page rank defined yet
UNASSIGNED_PAGE_RANK = 11
; threshold for when a complex proof is considered to be supporting a fact
CONFIRMATION_THRESHOLD = 0.5
;return all websites even the website has no proof
DISPLAY_WEBSITES_WITH_NO_PROOF = false

WORDNET_DICTIONARY = /opt/wordnet2


[settings]
; this is used to get the labels of the resources from the training models
RESOURCE_LABEL = http://www.w3.org/2000/01/rdf-schema#label
; training mode or demo mode (training generates arff files, demo mode outputs confidence scores)
TRAINING_MODE = false
; time period searcher: occurrence, global, domain
TIME_PERIOD_SEARCHER = domain
; tiny, small, medium, large
context-size = tiny
